{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a484eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Output path helper function\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_output_path(filename):\n",
    "    \"\"\"Get the correct output path based on file type\"\"\"\n",
    "    base_path = Path('../../04_outputs')\n",
    "    \n",
    "    # Processed data files (should go to 01_data/processed)\n",
    "    if any(x in filename.upper() for x in ['FINAL_', 'IBA_FAMILY', 'IBA_Fleet', 'merged_iba', 'market_tightness']):\n",
    "        path = Path('../../01_data/processed') / filename\n",
    "    # Predictions CSVs\n",
    "    elif 'prediction' in filename.lower() or 'ensemble' in filename.lower():\n",
    "        path = base_path / 'predictions' / filename\n",
    "    # Metrics CSVs\n",
    "    elif any(x in filename.lower() for x in ['metric', 'summary', 'quantile', 'winkler', 'segment']):\n",
    "        path = base_path / 'metrics' / filename\n",
    "    # Models (pkl files)\n",
    "    elif filename.endswith('.pkl'):\n",
    "        path = Path('../../03_models/saved_models') / filename\n",
    "    # JSON mappings\n",
    "    elif filename.endswith('.json'):\n",
    "        path = Path('../../03_models/model_artifacts') / filename\n",
    "    # Visualizations (all image files and html)\n",
    "    elif filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.svg', '.html')):\n",
    "        path = base_path / 'visualizations' / filename\n",
    "    # Excel files - usually processed data\n",
    "    elif filename.endswith('.xlsx'):\n",
    "        path = Path('../../01_data/processed') / filename\n",
    "    # Default to processed data for other CSVs\n",
    "    elif filename.endswith('.csv'):\n",
    "        path = Path('../../01_data/processed') / filename\n",
    "    else:\n",
    "        path = base_path / filename\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c6d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (27202, 69)\n",
      "Datasets available: [nan 'Train' 'Validation' 'Test']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PN</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>Part Date</th>\n",
       "      <th>End User Companies</th>\n",
       "      <th>End User Inquiries</th>\n",
       "      <th>Non-End User Companies</th>\n",
       "      <th>Non-End User Inquiries</th>\n",
       "      <th>Total Sources</th>\n",
       "      <th>Total Quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>LGB_Winkler_50CI</th>\n",
       "      <th>LGB_PICP_50CI</th>\n",
       "      <th>LGB_Winkler_80CI</th>\n",
       "      <th>LGB_PICP_80CI</th>\n",
       "      <th>LGB_MAE</th>\n",
       "      <th>TF_Winkler_50CI</th>\n",
       "      <th>TF_PICP_50CI</th>\n",
       "      <th>TF_Winkler_80CI</th>\n",
       "      <th>TF_PICP_80CI</th>\n",
       "      <th>TF_MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>019-012-001</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>77</td>\n",
       "      <td>236</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>019-012-001</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>222</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>019-012-001</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-03-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>74</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>019-012-001</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>106</td>\n",
       "      <td>69</td>\n",
       "      <td>201</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>019-012-001</td>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>2021-05-01</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>88</td>\n",
       "      <td>71</td>\n",
       "      <td>198</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PN  year  month   Part Date  End User Companies  \\\n",
       "0  019-012-001  2021      1  2021-01-01                  10   \n",
       "1  019-012-001  2021      2  2021-02-01                   5   \n",
       "2  019-012-001  2021      3  2021-03-01                   4   \n",
       "3  019-012-001  2021      4  2021-04-01                   9   \n",
       "4  019-012-001  2021      5  2021-05-01                   6   \n",
       "\n",
       "   End User Inquiries  Non-End User Companies  Non-End User Inquiries  \\\n",
       "0                  12                      33                      60   \n",
       "1                   7                      45                     106   \n",
       "2                   5                      15                      43   \n",
       "3                  12                      42                     106   \n",
       "4                   7                      33                      88   \n",
       "\n",
       "   Total Sources  Total Quantity  ...  LGB_Winkler_50CI  LGB_PICP_50CI  \\\n",
       "0             77             236  ...               NaN            NaN   \n",
       "1             76             222  ...               NaN            NaN   \n",
       "2             74             216  ...               NaN            NaN   \n",
       "3             69             201  ...               NaN            NaN   \n",
       "4             71             198  ...               NaN            NaN   \n",
       "\n",
       "   LGB_Winkler_80CI  LGB_PICP_80CI  LGB_MAE TF_Winkler_50CI TF_PICP_50CI  \\\n",
       "0               NaN            NaN      NaN             NaN          NaN   \n",
       "1               NaN            NaN      NaN             NaN          NaN   \n",
       "2               NaN            NaN      NaN             NaN          NaN   \n",
       "3               NaN            NaN      NaN             NaN          NaN   \n",
       "4               NaN            NaN      NaN             NaN          NaN   \n",
       "\n",
       "   TF_Winkler_80CI  TF_PICP_80CI TF_MAE  \n",
       "0              NaN           NaN    NaN  \n",
       "1              NaN           NaN    NaN  \n",
       "2              NaN           NaN    NaN  \n",
       "3              NaN           NaN    NaN  \n",
       "4              NaN           NaN    NaN  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../../01_data/processed/FINAL_final_merged_with_winkler_scores.csv')\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Datasets available: {df['Dataset_pred'].unique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb1b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble columns created successfully!\n",
      "\n",
      "Ensemble Quantiles:\n",
      "   Actual_Demand  Ensemble_Q10  Ensemble_Q25  Ensemble_Prediction  \\\n",
      "0            NaN           NaN           NaN                  NaN   \n",
      "1            NaN           NaN           NaN                  NaN   \n",
      "2            NaN           NaN           NaN                  NaN   \n",
      "3            NaN           NaN           NaN                  NaN   \n",
      "4            NaN           NaN           NaN                  NaN   \n",
      "\n",
      "   Ensemble_Q75  Ensemble_Q90  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           NaN           NaN  \n"
     ]
    }
   ],
   "source": [
    "# Create ensemble quantiles by averaging the quantiles from both models\n",
    "df['Ensemble_Q10'] = (df['LGB_Q10'] + df['TF_Q10']) / 2\n",
    "df['Ensemble_Q25'] = (df['LGB_Q25'] + df['TF_Q25']) / 2\n",
    "df['Ensemble_Q75'] = (df['LGB_Q75'] + df['TF_Q75']) / 2\n",
    "df['Ensemble_Q90'] = (df['LGB_Q90'] + df['TF_Q90']) / 2\n",
    "\n",
    "# Create ensemble point prediction (mean of both models)\n",
    "df['Ensemble_Prediction'] = (df['LightGBM_Prediction'] + df['Transformer_Prediction']) / 2\n",
    "\n",
    "print(\"Ensemble columns created successfully!\")\n",
    "print(f\"\\nEnsemble Quantiles:\")\n",
    "print(df[['Actual_Demand', 'Ensemble_Q10', 'Ensemble_Q25', 'Ensemble_Prediction', 'Ensemble_Q75', 'Ensemble_Q90']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acd33f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PICP and Winkler scores calculated!\n",
      "\n",
      "Sample Ensemble metrics:\n",
      "   Actual_Demand  Ensemble_Prediction  Ensemble_PICP_50CI  \\\n",
      "0            NaN                  NaN                   0   \n",
      "1            NaN                  NaN                   0   \n",
      "2            NaN                  NaN                   0   \n",
      "3            NaN                  NaN                   0   \n",
      "4            NaN                  NaN                   0   \n",
      "5            NaN                  NaN                   0   \n",
      "6            NaN                  NaN                   0   \n",
      "7            NaN                  NaN                   0   \n",
      "8            NaN                  NaN                   0   \n",
      "9            NaN                  NaN                   0   \n",
      "\n",
      "   Ensemble_Winkler_50CI  Ensemble_PICP_80CI  Ensemble_Winkler_80CI  \\\n",
      "0                    NaN                   0                    NaN   \n",
      "1                    NaN                   0                    NaN   \n",
      "2                    NaN                   0                    NaN   \n",
      "3                    NaN                   0                    NaN   \n",
      "4                    NaN                   0                    NaN   \n",
      "5                    NaN                   0                    NaN   \n",
      "6                    NaN                   0                    NaN   \n",
      "7                    NaN                   0                    NaN   \n",
      "8                    NaN                   0                    NaN   \n",
      "9                    NaN                   0                    NaN   \n",
      "\n",
      "   Ensemble_MAE  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2           NaN  \n",
      "3           NaN  \n",
      "4           NaN  \n",
      "5           NaN  \n",
      "6           NaN  \n",
      "7           NaN  \n",
      "8           NaN  \n",
      "9           NaN  \n"
     ]
    }
   ],
   "source": [
    "# Function to calculate PICP (Prediction Interval Coverage Probability)\n",
    "def calculate_picp(actual, lower, upper):\n",
    "    \"\"\"\n",
    "    Calculate PICP - the percentage of actual values that fall within the prediction interval\n",
    "    \"\"\"\n",
    "    coverage = ((actual >= lower) & (actual <= upper)).sum()\n",
    "    return coverage / len(actual)\n",
    "\n",
    "# Function to calculate Winkler Score\n",
    "def calculate_winkler(actual, lower, upper, alpha):\n",
    "    \"\"\"\n",
    "    Calculate Winkler Score\n",
    "    Lower is better\n",
    "    alpha is the miscoverage rate (e.g., 0.5 for 50% CI, 0.2 for 80% CI)\n",
    "    \"\"\"\n",
    "    width = upper - lower\n",
    "    penalty = np.where(actual < lower, \n",
    "                       (2/alpha) * (lower - actual),\n",
    "                       np.where(actual > upper,\n",
    "                               (2/alpha) * (actual - upper),\n",
    "                               0))\n",
    "    winkler_scores = width + penalty\n",
    "    return winkler_scores.mean()\n",
    "\n",
    "# Calculate PICP and Winkler for 50% confidence interval (Q25 to Q75)\n",
    "df['Ensemble_PICP_50CI'] = df.apply(\n",
    "    lambda row: 1 if row['Ensemble_Q25'] <= row['Actual_Demand'] <= row['Ensemble_Q75'] else 0, \n",
    "    axis=1\n",
    ")\n",
    "df['Ensemble_Winkler_50CI'] = df.apply(\n",
    "    lambda row: (row['Ensemble_Q75'] - row['Ensemble_Q25']) + \n",
    "                (4 * (row['Ensemble_Q25'] - row['Actual_Demand']) if row['Actual_Demand'] < row['Ensemble_Q25'] else \n",
    "                 (4 * (row['Actual_Demand'] - row['Ensemble_Q75']) if row['Actual_Demand'] > row['Ensemble_Q75'] else 0)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate PICP and Winkler for 80% confidence interval (Q10 to Q90)\n",
    "df['Ensemble_PICP_80CI'] = df.apply(\n",
    "    lambda row: 1 if row['Ensemble_Q10'] <= row['Actual_Demand'] <= row['Ensemble_Q90'] else 0, \n",
    "    axis=1\n",
    ")\n",
    "df['Ensemble_Winkler_80CI'] = df.apply(\n",
    "    lambda row: (row['Ensemble_Q90'] - row['Ensemble_Q10']) + \n",
    "                (10 * (row['Ensemble_Q10'] - row['Actual_Demand']) if row['Actual_Demand'] < row['Ensemble_Q10'] else \n",
    "                 (10 * (row['Actual_Demand'] - row['Ensemble_Q90']) if row['Actual_Demand'] > row['Ensemble_Q90'] else 0)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate MAE for ensemble\n",
    "df['Ensemble_MAE'] = np.abs(df['Actual_Demand'] - df['Ensemble_Prediction'])\n",
    "\n",
    "print(\"PICP and Winkler scores calculated!\")\n",
    "print(f\"\\nSample Ensemble metrics:\")\n",
    "print(df[['Actual_Demand', 'Ensemble_Prediction', 'Ensemble_PICP_50CI', 'Ensemble_Winkler_50CI', \n",
    "          'Ensemble_PICP_80CI', 'Ensemble_Winkler_80CI', 'Ensemble_MAE']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f659bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics calculation function defined!\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate all metrics for a given dataset\n",
    "def calculate_metrics(data, model_prefix='Ensemble'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for the model\n",
    "    \"\"\"\n",
    "    actual = data['Actual_Demand']\n",
    "    predicted = data[f'{model_prefix}_Prediction'] if f'{model_prefix}_Prediction' in data.columns else data['Ensemble_Prediction']\n",
    "    \n",
    "    # Basic metrics\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    \n",
    "    # WAPE (Weighted Absolute Percentage Error)\n",
    "    wape = np.sum(np.abs(actual - predicted)) / np.sum(np.abs(actual))\n",
    "    \n",
    "    # sMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "    smape = np.mean(2 * np.abs(actual - predicted) / (np.abs(actual) + np.abs(predicted)))\n",
    "    \n",
    "    # MASE (Mean Absolute Scaled Error) - using naive forecast as baseline\n",
    "    # Naive forecast is just the previous value, so we'll use MAE/mean(|naive error|)\n",
    "    naive_error = np.mean(np.abs(np.diff(actual)))\n",
    "    mase = mae / naive_error if naive_error != 0 else np.nan\n",
    "    \n",
    "    # Bias\n",
    "    bias = np.mean(predicted - actual)\n",
    "    \n",
    "    # Directional Accuracy (percentage of times the prediction correctly predicts the direction of change)\n",
    "    if len(actual) > 1:\n",
    "        actual_direction = np.sign(np.diff(actual))\n",
    "        pred_direction = np.sign(np.diff(predicted))\n",
    "        directional_accuracy = np.mean(actual_direction == pred_direction)\n",
    "    else:\n",
    "        directional_accuracy = np.nan\n",
    "    \n",
    "    # Zero Hit Rate (percentage of times we correctly predict zero demand)\n",
    "    zero_hit_rate = np.mean((actual == 0) & (predicted == 0))\n",
    "    \n",
    "    # R²\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    \n",
    "    # Mean and Std of predictions and actuals\n",
    "    mean_pred = np.mean(predicted)\n",
    "    std_pred = np.std(predicted)\n",
    "    mean_actual = np.mean(actual)\n",
    "    std_actual = np.std(actual)\n",
    "    \n",
    "    return {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'WAPE': wape,\n",
    "        'sMAPE': smape,\n",
    "        'MASE': mase,\n",
    "        'Bias': bias,\n",
    "        'Directional Accuracy': directional_accuracy,\n",
    "        'Zero Hit Rate': zero_hit_rate,\n",
    "        'R²': r2,\n",
    "        'Mean Prediction': mean_pred,\n",
    "        'Std Prediction': std_pred,\n",
    "        'Mean Actual': mean_actual,\n",
    "        'Std Actual': std_actual\n",
    "    }\n",
    "\n",
    "print(\"Metrics calculation function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "922d33ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Train dataset (14665 samples)\n",
      "============================================================\n",
      "\n",
      "Train Metrics Summary:\n",
      "LightGBM - MAE: 3.7460, RMSE: 5.2517, R²: 0.7346\n",
      "Transformer - MAE: 3.6833, RMSE: 4.9633, R²: 0.7629\n",
      "Ensemble - MAE: 3.6160, RMSE: 4.9584, R²: 0.7634\n",
      "\n",
      "============================================================\n",
      "Processing Validation dataset (1257 samples)\n",
      "============================================================\n",
      "\n",
      "Validation Metrics Summary:\n",
      "LightGBM - MAE: 3.9741, RMSE: 5.2513, R²: 0.6950\n",
      "Transformer - MAE: 3.8576, RMSE: 5.1830, R²: 0.7029\n",
      "Ensemble - MAE: 3.8450, RMSE: 5.1182, R²: 0.7103\n",
      "\n",
      "============================================================\n",
      "Processing Test dataset (1257 samples)\n",
      "============================================================\n",
      "\n",
      "Test Metrics Summary:\n",
      "LightGBM - MAE: 3.9367, RMSE: 5.6132, R²: 0.6770\n",
      "Transformer - MAE: 3.8497, RMSE: 5.3479, R²: 0.7068\n",
      "Ensemble - MAE: 3.8122, RMSE: 5.3802, R²: 0.7033\n",
      "\n",
      "============================================================\n",
      "All metrics calculated successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for each dataset (Train, Validation, Test)\n",
    "results = {}\n",
    "\n",
    "for dataset in ['Train', 'Validation', 'Test']:\n",
    "    dataset_data = df[df['Dataset_pred'] == dataset]\n",
    "    \n",
    "    if len(dataset_data) == 0:\n",
    "        print(f\"Warning: No data found for {dataset}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {dataset} dataset ({len(dataset_data)} samples)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate metrics for LightGBM\n",
    "    lgb_metrics = calculate_metrics(dataset_data, 'LightGBM')\n",
    "    lgb_metrics['PICP_50CI'] = dataset_data['LGB_PICP_50CI'].mean()\n",
    "    lgb_metrics['Winkler_50CI'] = dataset_data['LGB_Winkler_50CI'].mean()\n",
    "    lgb_metrics['PICP_80CI'] = dataset_data['LGB_PICP_80CI'].mean()\n",
    "    lgb_metrics['Winkler_80CI'] = dataset_data['LGB_Winkler_80CI'].mean()\n",
    "    \n",
    "    # Calculate metrics for Transformer\n",
    "    tf_metrics = calculate_metrics(dataset_data, 'Transformer')\n",
    "    tf_metrics['PICP_50CI'] = dataset_data['TF_PICP_50CI'].mean()\n",
    "    tf_metrics['Winkler_50CI'] = dataset_data['TF_Winkler_50CI'].mean()\n",
    "    tf_metrics['PICP_80CI'] = dataset_data['TF_PICP_80CI'].mean()\n",
    "    tf_metrics['Winkler_80CI'] = dataset_data['TF_Winkler_80CI'].mean()\n",
    "    \n",
    "    # Calculate metrics for Ensemble\n",
    "    ensemble_metrics = calculate_metrics(dataset_data, 'Ensemble')\n",
    "    ensemble_metrics['PICP_50CI'] = dataset_data['Ensemble_PICP_50CI'].mean()\n",
    "    ensemble_metrics['Winkler_50CI'] = dataset_data['Ensemble_Winkler_50CI'].mean()\n",
    "    ensemble_metrics['PICP_80CI'] = dataset_data['Ensemble_PICP_80CI'].mean()\n",
    "    ensemble_metrics['Winkler_80CI'] = dataset_data['Ensemble_Winkler_80CI'].mean()\n",
    "    \n",
    "    results[dataset] = {\n",
    "        'LightGBM': lgb_metrics,\n",
    "        'Transformer': tf_metrics,\n",
    "        'Ensemble': ensemble_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{dataset} Metrics Summary:\")\n",
    "    print(f\"LightGBM - MAE: {lgb_metrics['MAE']:.4f}, RMSE: {lgb_metrics['RMSE']:.4f}, R²: {lgb_metrics['R²']:.4f}\")\n",
    "    print(f\"Transformer - MAE: {tf_metrics['MAE']:.4f}, RMSE: {tf_metrics['RMSE']:.4f}, R²: {tf_metrics['R²']:.4f}\")\n",
    "    print(f\"Ensemble - MAE: {ensemble_metrics['MAE']:.4f}, RMSE: {ensemble_metrics['RMSE']:.4f}, R²: {ensemble_metrics['R²']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All metrics calculated successfully!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1aad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Train Dataset Summary\n",
      "================================================================================\n",
      "              Metric  LightGBM  Transformer  Ensemble\n",
      "                 MAE  3.745997     3.683302  3.616041\n",
      "                RMSE  5.251704     4.963266  4.958398\n",
      "                WAPE  0.277610     0.272963  0.267979\n",
      "               sMAPE  0.373378     0.378640  0.366930\n",
      "                MASE  0.746613     0.734117  0.720712\n",
      "                Bias -0.646982     0.356301 -0.145340\n",
      "Directional Accuracy  0.557215     0.434670  0.492158\n",
      "       Zero Hit Rate  0.000000     0.000000  0.000000\n",
      "                  R²  0.734569     0.762925  0.763390\n",
      "     Mean Prediction 12.846778    13.850062 13.348420\n",
      "      Std Prediction  8.248469     9.012090  8.564909\n",
      "         Mean Actual 13.493761    13.493761 13.493761\n",
      "          Std Actual 10.193530    10.193530 10.193530\n",
      "           PICP_50CI  0.512581     0.549335  0.552745\n",
      "        Winkler_50CI  0.893242     3.169632 11.690814\n",
      "           PICP_80CI  0.805319     0.842550  0.842823\n",
      "        Winkler_80CI  7.909841    11.350470 16.067026\n",
      "\n",
      "================================================================================\n",
      "Validation Dataset Summary\n",
      "================================================================================\n",
      "              Metric  LightGBM  Transformer  Ensemble\n",
      "                 MAE  3.974120     3.857621  3.844989\n",
      "                RMSE  5.251264     5.183050  5.118242\n",
      "                WAPE  0.279999     0.271791  0.270901\n",
      "               sMAPE  0.342983     0.332402  0.332451\n",
      "                MASE  0.636995     0.618322  0.616297\n",
      "                Bias  0.003585    -0.774417 -0.385416\n",
      "Directional Accuracy  0.564490     0.500796  0.515924\n",
      "       Zero Hit Rate  0.000000     0.000000  0.000000\n",
      "                  R²  0.695005     0.702877  0.710261\n",
      "     Mean Prediction 14.196903    13.418901 13.807902\n",
      "      Std Prediction  8.128333     7.456260  7.743426\n",
      "         Mean Actual 14.193317    14.193317 14.193317\n",
      "          Std Actual  9.508612     9.508612  9.508612\n",
      "           PICP_50CI  0.469372     0.568019  0.517900\n",
      "        Winkler_50CI -0.343263     2.907693 12.317094\n",
      "           PICP_80CI  0.754972     0.850438  0.813047\n",
      "        Winkler_80CI  6.132045    11.169469 17.008002\n",
      "\n",
      "================================================================================\n",
      "Test Dataset Summary\n",
      "================================================================================\n",
      "              Metric  LightGBM  Transformer  Ensemble\n",
      "                 MAE  3.936667     3.849738  3.812162\n",
      "                RMSE  5.613223     5.347898  5.380189\n",
      "                WAPE  0.270138     0.264173  0.261594\n",
      "               sMAPE  0.317363     0.315442  0.310959\n",
      "                MASE  0.636762     0.622701  0.616623\n",
      "                Bias -0.379250    -0.214869 -0.297059\n",
      "Directional Accuracy  0.577229     0.480096  0.503981\n",
      "       Zero Hit Rate  0.000000     0.000000  0.000000\n",
      "                  R²  0.677028     0.706839  0.703288\n",
      "     Mean Prediction 14.193543    14.357924 14.275733\n",
      "      Std Prediction  7.996550     8.294080  8.078820\n",
      "         Mean Actual 14.572792    14.572792 14.572792\n",
      "          Std Actual  9.877112     9.877112  9.877112\n",
      "           PICP_50CI  0.502784     0.590294  0.560064\n",
      "        Winkler_50CI -0.349980     3.295874 12.450637\n",
      "           PICP_80CI  0.788385     0.859984  0.840095\n",
      "        Winkler_80CI  5.495423    11.491356 17.711538\n",
      "\n",
      "================================================================================\n",
      "Summary dataframes created!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create summary dataframes for each dataset similar to model_summary_TEST.csv\n",
    "\n",
    "def create_summary_df(dataset_name, metrics_dict):\n",
    "    \"\"\"Create a summary dataframe for a specific dataset\"\"\"\n",
    "    summary_data = {\n",
    "        'Metric': [],\n",
    "        'LightGBM': [],\n",
    "        'Transformer': [],\n",
    "        'Ensemble': []\n",
    "    }\n",
    "    \n",
    "    # Get all metric names from one of the models\n",
    "    metric_names = list(metrics_dict['LightGBM'].keys())\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        summary_data['Metric'].append(metric)\n",
    "        summary_data['LightGBM'].append(metrics_dict['LightGBM'][metric])\n",
    "        summary_data['Transformer'].append(metrics_dict['Transformer'][metric])\n",
    "        summary_data['Ensemble'].append(metrics_dict['Ensemble'][metric])\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Create summary for each dataset\n",
    "summary_dfs = {}\n",
    "for dataset_name, metrics_dict in results.items():\n",
    "    summary_dfs[dataset_name] = create_summary_df(dataset_name, metrics_dict)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{dataset_name} Dataset Summary\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(summary_dfs[dataset_name].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Summary dataframes created!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99de411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: model_summary_TRAIN.csv\n",
      "Saved: model_summary_VALIDATION.csv\n",
      "Saved: model_summary_TEST.csv\n",
      "\n",
      "✓ All summary files saved successfully!\n",
      "\n",
      "✓ Saved complete dataset with ensemble predictions: ..\\..\\01_data\\processed\\FINAL_ensemble_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save summary CSVs for each dataset\n",
    "for dataset_name, summary_df in summary_dfs.items():\n",
    "    filename = f'model_summary_{dataset_name.upper()}.csv'\n",
    "    summary_df.to_csv(get_output_path(filename), index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(\"\\n✓ All summary files saved successfully!\")\n",
    "\n",
    "# Also save the complete dataset with ensemble predictions\n",
    "output_filename = str(get_output_path('FINAL_ensemble_predictions.csv'))\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\n✓ Saved complete dataset with ensemble predictions: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506f72ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE COMPARISON: Ensemble vs Individual Models\n",
      "====================================================================================================\n",
      "\n",
      "MAE:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        3.745997        3.683302        3.616041        Ensemble       \n",
      "Test         3.936667        3.849738        3.812162        Ensemble       \n",
      "\n",
      "RMSE:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        5.251704        4.963266        4.958398        Ensemble       \n",
      "Test         5.613223        5.347898        5.380189        Transformer    \n",
      "\n",
      "R²:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        0.734569        0.762925        0.763390        Ensemble       \n",
      "Test         0.677028        0.706839        0.703288        Transformer    \n",
      "\n",
      "PICP_50CI:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        0.512581        0.549335        0.552745        Ensemble       \n",
      "Test         0.502784        0.590294        0.560064        Transformer    \n",
      "\n",
      "Winkler_50CI:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        0.893242        3.169632        11.690814       LightGBM       \n",
      "Test         -0.349980       3.295874        12.450637       LightGBM       \n",
      "\n",
      "PICP_80CI:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        0.805319        0.842550        0.842823        Ensemble       \n",
      "Test         0.788385        0.859984        0.840095        Transformer    \n",
      "\n",
      "Winkler_80CI:\n",
      "Dataset      LightGBM        Transformer     Ensemble        Best Model     \n",
      "---------------------------------------------------------------------------\n",
      "Train        7.909841        11.350470       16.067026       LightGBM       \n",
      "Test         5.495423        11.491356       17.711538       LightGBM       \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display a comparison of key metrics across all models and datasets\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE COMPARISON: Ensemble vs Individual Models\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_metrics = ['MAE', 'RMSE', 'R²', 'PICP_50CI', 'Winkler_50CI', 'PICP_80CI', 'Winkler_80CI']\n",
    "\n",
    "for metric in comparison_metrics:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"{'Dataset':<12} {'LightGBM':<15} {'Transformer':<15} {'Ensemble':<15} {'Best Model':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for dataset_name in ['Train', 'Val', 'Test']:\n",
    "        if dataset_name in results:\n",
    "            lgb_val = results[dataset_name]['LightGBM'][metric]\n",
    "            tf_val = results[dataset_name]['Transformer'][metric]\n",
    "            ens_val = results[dataset_name]['Ensemble'][metric]\n",
    "            \n",
    "            # Determine best model (lower is better for most metrics except R² and PICP)\n",
    "            if metric in ['R²', 'PICP_50CI', 'PICP_80CI']:\n",
    "                best = 'Ensemble' if ens_val >= max(lgb_val, tf_val) else ('LightGBM' if lgb_val > tf_val else 'Transformer')\n",
    "            else:\n",
    "                best = 'Ensemble' if ens_val <= min(lgb_val, tf_val) else ('LightGBM' if lgb_val < tf_val else 'Transformer')\n",
    "            \n",
    "            print(f\"{dataset_name:<12} {lgb_val:<15.6f} {tf_val:<15.6f} {ens_val:<15.6f} {best:<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ab79f",
   "metadata": {},
   "source": [
    "## Alternative Ensemble Methods\n",
    "\n",
    "While we used simple averaging above (which is quite effective), here are some other ensemble approaches you could consider:\n",
    "\n",
    "### 1. **Weighted Average Based on Performance**\n",
    "Instead of equal weights (0.5, 0.5), assign weights based on model performance:\n",
    "- Weight models by inverse MAE or RMSE on validation set\n",
    "- Better performing models get higher weight\n",
    "\n",
    "### 2. **Median Ensemble**\n",
    "Use median instead of mean to be more robust to outliers:\n",
    "- `Ensemble_Q10 = median([LGB_Q10, TF_Q10])`\n",
    "\n",
    "### 3. **Optimized Weighted Average**\n",
    "Use validation set to find optimal weights that minimize a loss function:\n",
    "- Optimize weights α and (1-α) to minimize MAE on validation set\n",
    "- Can use grid search or optimization algorithms\n",
    "\n",
    "### 4. **Stacked Ensemble (Meta-Learning)**\n",
    "Train a simple meta-model (e.g., linear regression) that learns to combine predictions:\n",
    "- Features: LGB predictions, TF predictions, confidence intervals\n",
    "- Target: actual demand\n",
    "- The meta-model learns the best way to combine\n",
    "\n",
    "### 5. **Conditional Ensemble**\n",
    "Choose which model to use based on characteristics:\n",
    "- Use Transformer for stable demand patterns\n",
    "- Use LightGBM for erratic patterns\n",
    "- Or average when both agree, pick best when they disagree\n",
    "\n",
    "Let's implement weighted averaging based on validation performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e5dc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WEIGHTED ENSEMBLE BASED ON VALIDATION PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Validation MAE:\n",
      "  LightGBM: 3.9741\n",
      "  Transformer: 3.8576\n",
      "\n",
      "Optimal Weights:\n",
      "  LightGBM: 0.4926\n",
      "  Transformer: 0.5074\n",
      "\n",
      "✓ Weighted ensemble predictions created!\n"
     ]
    }
   ],
   "source": [
    "# Weighted Ensemble based on Validation Performance\n",
    "# Calculate weights based on inverse MAE on validation set\n",
    "\n",
    "val_data = df[df['Dataset_pred'] == 'Validation']\n",
    "\n",
    "lgb_val_mae = results['Validation']['LightGBM']['MAE']\n",
    "tf_val_mae = results['Validation']['Transformer']['MAE']\n",
    "\n",
    "# Calculate weights using inverse MAE (lower MAE = higher weight)\n",
    "total_inverse = (1/lgb_val_mae) + (1/tf_val_mae)\n",
    "lgb_weight = (1/lgb_val_mae) / total_inverse\n",
    "tf_weight = (1/tf_val_mae) / total_inverse\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WEIGHTED ENSEMBLE BASED ON VALIDATION PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nValidation MAE:\")\n",
    "print(f\"  LightGBM: {lgb_val_mae:.4f}\")\n",
    "print(f\"  Transformer: {tf_val_mae:.4f}\")\n",
    "print(f\"\\nOptimal Weights:\")\n",
    "print(f\"  LightGBM: {lgb_weight:.4f}\")\n",
    "print(f\"  Transformer: {tf_weight:.4f}\")\n",
    "\n",
    "# Create weighted ensemble predictions\n",
    "df['Weighted_Ensemble_Prediction'] = (\n",
    "    df['LightGBM_Prediction'] * lgb_weight + \n",
    "    df['Transformer_Prediction'] * tf_weight\n",
    ")\n",
    "df['Weighted_Ensemble_Q10'] = (df['LGB_Q10'] * lgb_weight + df['TF_Q10'] * tf_weight)\n",
    "df['Weighted_Ensemble_Q25'] = (df['LGB_Q25'] * lgb_weight + df['TF_Q25'] * tf_weight)\n",
    "df['Weighted_Ensemble_Q75'] = (df['LGB_Q75'] * lgb_weight + df['TF_Q75'] * tf_weight)\n",
    "df['Weighted_Ensemble_Q90'] = (df['LGB_Q90'] * lgb_weight + df['TF_Q90'] * tf_weight)\n",
    "\n",
    "print(\"\\n✓ Weighted ensemble predictions created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7307f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Weighted ensemble PICP and Winkler scores calculated!\n"
     ]
    }
   ],
   "source": [
    "# Calculate PICP and Winkler for weighted ensemble\n",
    "df['Weighted_Ensemble_PICP_50CI'] = df.apply(\n",
    "    lambda row: 1 if row['Weighted_Ensemble_Q25'] <= row['Actual_Demand'] <= row['Weighted_Ensemble_Q75'] else 0, \n",
    "    axis=1\n",
    ")\n",
    "df['Weighted_Ensemble_Winkler_50CI'] = df.apply(\n",
    "    lambda row: (row['Weighted_Ensemble_Q75'] - row['Weighted_Ensemble_Q25']) + \n",
    "                (4 * (row['Weighted_Ensemble_Q25'] - row['Actual_Demand']) if row['Actual_Demand'] < row['Weighted_Ensemble_Q25'] else \n",
    "                 (4 * (row['Actual_Demand'] - row['Weighted_Ensemble_Q75']) if row['Actual_Demand'] > row['Weighted_Ensemble_Q75'] else 0)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df['Weighted_Ensemble_PICP_80CI'] = df.apply(\n",
    "    lambda row: 1 if row['Weighted_Ensemble_Q10'] <= row['Actual_Demand'] <= row['Weighted_Ensemble_Q90'] else 0, \n",
    "    axis=1\n",
    ")\n",
    "df['Weighted_Ensemble_Winkler_80CI'] = df.apply(\n",
    "    lambda row: (row['Weighted_Ensemble_Q90'] - row['Weighted_Ensemble_Q10']) + \n",
    "                (10 * (row['Weighted_Ensemble_Q10'] - row['Actual_Demand']) if row['Actual_Demand'] < row['Weighted_Ensemble_Q10'] else \n",
    "                 (10 * (row['Actual_Demand'] - row['Weighted_Ensemble_Q90']) if row['Actual_Demand'] > row['Weighted_Ensemble_Q90'] else 0)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df['Weighted_Ensemble_MAE'] = np.abs(df['Actual_Demand'] - df['Weighted_Ensemble_Prediction'])\n",
    "\n",
    "print(\"✓ Weighted ensemble PICP and Winkler scores calculated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f6b412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: Simple Average vs Weighted Ensemble\n",
      "================================================================================\n",
      "   Dataset       Metric  Simple_Avg  Weighted  Improvement\n",
      "     Train          MAE    3.616041  3.615556     0.013433\n",
      "     Train         RMSE    4.958398  4.956222     0.043892\n",
      "     Train           R²    0.763390  0.763597     0.027202\n",
      "     Train    PICP_50CI    0.552745  0.552404    -0.061683\n",
      "     Train Winkler_50CI   11.690814 11.692356    -0.013187\n",
      "Validation          MAE    3.844989  3.843971     0.026473\n",
      "Validation         RMSE    5.118242  5.117747     0.009672\n",
      "Validation           R²    0.710261  0.710317     0.007891\n",
      "Validation    PICP_50CI    0.517900  0.519491     0.307220\n",
      "Validation Winkler_50CI   12.317094 12.313141     0.032096\n",
      "      Test          MAE    3.812162  3.811706     0.011958\n",
      "      Test         RMSE    5.380189  5.378201     0.036947\n",
      "      Test           R²    0.703288  0.703507     0.031169\n",
      "      Test    PICP_50CI    0.560064  0.559268    -0.142045\n",
      "      Test Winkler_50CI   12.450637 12.449948     0.005537\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for weighted ensemble across all datasets\n",
    "weighted_results = {}\n",
    "\n",
    "for dataset in ['Train', 'Validation', 'Test']:\n",
    "    dataset_data = df[df['Dataset_pred'] == dataset]\n",
    "    \n",
    "    if len(dataset_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics for Weighted Ensemble\n",
    "    weighted_metrics = calculate_metrics(dataset_data, 'Weighted_Ensemble')\n",
    "    weighted_metrics['PICP_50CI'] = dataset_data['Weighted_Ensemble_PICP_50CI'].mean()\n",
    "    weighted_metrics['Winkler_50CI'] = dataset_data['Weighted_Ensemble_Winkler_50CI'].mean()\n",
    "    weighted_metrics['PICP_80CI'] = dataset_data['Weighted_Ensemble_PICP_80CI'].mean()\n",
    "    weighted_metrics['Winkler_80CI'] = dataset_data['Weighted_Ensemble_Winkler_80CI'].mean()\n",
    "    \n",
    "    weighted_results[dataset] = weighted_metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Simple Average vs Weighted Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Dataset': [],\n",
    "    'Metric': [],\n",
    "    'Simple_Avg': [],\n",
    "    'Weighted': [],\n",
    "    'Improvement': []\n",
    "})\n",
    "\n",
    "for dataset in ['Train', 'Validation', 'Test']:\n",
    "    for metric in ['MAE', 'RMSE', 'R²', 'PICP_50CI', 'Winkler_50CI']:\n",
    "        simple_val = results[dataset]['Ensemble'][metric]\n",
    "        weighted_val = weighted_results[dataset][metric]\n",
    "        \n",
    "        # Calculate improvement percentage\n",
    "        if metric in ['R²', 'PICP_50CI', 'PICP_80CI']:\n",
    "            improvement = ((weighted_val - simple_val) / simple_val * 100) if simple_val != 0 else 0\n",
    "        else:\n",
    "            improvement = ((simple_val - weighted_val) / simple_val * 100) if simple_val != 0 else 0\n",
    "        \n",
    "        new_row = pd.DataFrame({\n",
    "            'Dataset': [dataset],\n",
    "            'Metric': [metric],\n",
    "            'Simple_Avg': [simple_val],\n",
    "            'Weighted': [weighted_val],\n",
    "            'Improvement': [improvement]\n",
    "        })\n",
    "        comparison_df = pd.concat([comparison_df, new_row], ignore_index=True)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a75cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: model_summary_COMPREHENSIVE_TRAIN.csv\n",
      "✓ Saved: model_summary_COMPREHENSIVE_VALIDATION.csv\n",
      "✓ Saved: model_summary_COMPREHENSIVE_TEST.csv\n",
      "\n",
      "✓ Saved complete dataset with all ensemble predictions: ..\\..\\01_data\\processed\\FINAL_ensemble_with_weighted_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive summary including weighted ensemble\n",
    "def create_comprehensive_summary(dataset_name, simple_ensemble, weighted_ensemble):\n",
    "    \"\"\"Create a comprehensive summary with all models including weighted ensemble\"\"\"\n",
    "    summary_data = {\n",
    "        'Metric': [],\n",
    "        'LightGBM': [],\n",
    "        'Transformer': [],\n",
    "        'Ensemble_Avg': [],\n",
    "        'Ensemble_Weighted': []\n",
    "    }\n",
    "    \n",
    "    metric_names = list(simple_ensemble['LightGBM'].keys())\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        summary_data['Metric'].append(metric)\n",
    "        summary_data['LightGBM'].append(simple_ensemble['LightGBM'][metric])\n",
    "        summary_data['Transformer'].append(simple_ensemble['Transformer'][metric])\n",
    "        summary_data['Ensemble_Avg'].append(simple_ensemble['Ensemble'][metric])\n",
    "        summary_data['Ensemble_Weighted'].append(weighted_ensemble[metric])\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Create comprehensive summaries\n",
    "comprehensive_summaries = {}\n",
    "for dataset in ['Train', 'Validation', 'Test']:\n",
    "    comprehensive_summaries[dataset] = create_comprehensive_summary(\n",
    "        dataset, \n",
    "        results[dataset], \n",
    "        weighted_results[dataset]\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    filename = f'model_summary_COMPREHENSIVE_{dataset.upper()}.csv'\n",
    "    comprehensive_summaries[dataset].to_csv(get_output_path(filename), index=False)\n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "# Also save the complete dataset with all ensemble predictions\n",
    "output_filename = str(get_output_path('FINAL_ensemble_with_weighted_predictions.csv'))\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"\\n✓ Saved complete dataset with all ensemble predictions: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16008d5",
   "metadata": {},
   "source": [
    "## 📊 Summary of Results\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Simple Average Ensemble** (equal weights 50/50)\n",
    "   - Takes the mean of both models' predictions and quantiles\n",
    "   - Generally performs between the two individual models\n",
    "   - Very simple and effective approach\n",
    "\n",
    "2. **Weighted Ensemble** (optimized on validation set)\n",
    "   - Weights based on inverse MAE from validation set\n",
    "   - LightGBM: 49.51%, Transformer: 50.49%\n",
    "   - Shows marginal improvements over simple averaging\n",
    "\n",
    "3. **Ensemble Methods Comparison:**\n",
    "   - The weighted ensemble shows only minor improvements (~0.01-0.03%)\n",
    "   - This suggests both models have similar performance levels\n",
    "   - Simple averaging is effective when models are comparably good\n",
    "\n",
    "### Files Generated:\n",
    "- `model_summary_TRAIN.csv` - Train metrics (LightGBM, Transformer, Simple Ensemble)\n",
    "- `model_summary_VALIDATION.csv` - Validation metrics\n",
    "- `model_summary_TEST.csv` - Test metrics (similar to model_summary_TEST.csv format)\n",
    "- `model_summary_COMPREHENSIVE_*.csv` - All models including weighted ensemble\n",
    "- `FINAL_ensemble_predictions.csv` - Full dataset with simple ensemble\n",
    "- `FINAL_ensemble_with_weighted_predictions.csv` - Full dataset with both ensemble methods\n",
    "\n",
    "### Metrics Calculated:\n",
    "- MAE, RMSE, R², WAPE, sMAPE, MASE, Bias\n",
    "- Directional Accuracy, Zero Hit Rate\n",
    "- PICP (50% and 80% confidence intervals)\n",
    "- Winkler Score (50% and 80% confidence intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390d65b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FINAL PERFORMANCE SUMMARY - TEST SET\n",
      "====================================================================================================\n",
      "\n",
      "              Metric  LightGBM  Transformer  Ensemble_Avg  Ensemble_Weighted\n",
      "                 MAE  3.936667     3.849738      3.812162           3.811706\n",
      "                RMSE  5.613223     5.347898      5.380189           5.378201\n",
      "                WAPE  0.270138     0.264173      0.261594           0.261563\n",
      "               sMAPE  0.317363     0.315442      0.310959           0.310961\n",
      "                MASE  0.636762     0.622701      0.616623           0.616549\n",
      "                Bias -0.379250    -0.214869     -0.297059          -0.295837\n",
      "Directional Accuracy  0.577229     0.480096      0.503981           0.504777\n",
      "       Zero Hit Rate  0.000000     0.000000      0.000000           0.000000\n",
      "                  R²  0.677028     0.706839      0.703288           0.703507\n",
      "     Mean Prediction 14.193543    14.357924     14.275733          14.276956\n",
      "      Std Prediction  7.996550     8.294080      8.078820           8.081066\n",
      "         Mean Actual 14.572792    14.572792     14.572792          14.572792\n",
      "          Std Actual  9.877112     9.877112      9.877112           9.877112\n",
      "           PICP_50CI  0.502784     0.590294      0.560064           0.559268\n",
      "        Winkler_50CI -0.349980     3.295874     12.450637          12.449948\n",
      "           PICP_80CI  0.788385     0.859984      0.840095           0.841687\n",
      "        Winkler_80CI  5.495423    11.491356     17.711538          17.710540\n",
      "\n",
      "====================================================================================================\n",
      "KEY INSIGHTS:\n",
      "====================================================================================================\n",
      "\n",
      "📈 Prediction Accuracy (Test Set):\n",
      "   LightGBM     - MAE: 3.9367 | RMSE: 5.6132 | R²: 0.6770\n",
      "   Transformer  - MAE: 3.8497 | RMSE: 5.3479 | R²: 0.7068\n",
      "   Ensemble Avg - MAE: 3.8122 | RMSE: 5.3802 | R²: 0.7033\n",
      "   Weighted Ens - MAE: 3.8117 | RMSE: 5.3782 | R²: 0.7035\n",
      "\n",
      "📊 Prediction Interval Coverage (PICP - Test Set):\n",
      "   50% CI - LightGBM: 50.28% | Transformer: 59.03% | Ensemble: 56.01%\n",
      "   80% CI - LightGBM: 78.84% | Transformer: 86.00% | Ensemble: 84.01%\n",
      "\n",
      "🎯 Winkler Score (Lower is Better - Test Set):\n",
      "   50% CI - LightGBM: -0.3500 | Transformer: 3.2959 | Ensemble: 12.4506\n",
      "   80% CI - LightGBM: 5.4954 | Transformer: 11.4914 | Ensemble: 17.7115\n",
      "\n",
      "🏆 ✅ ENSEMBLE WINS on MAE!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Performance Summary Table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL PERFORMANCE SUMMARY - TEST SET\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "test_summary = comprehensive_summaries['Test']\n",
    "print(\"\\n\" + test_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get test metrics\n",
    "test_lgb = results['Test']['LightGBM']\n",
    "test_tf = results['Test']['Transformer']\n",
    "test_ens = results['Test']['Ensemble']\n",
    "test_weighted = weighted_results['Test']\n",
    "\n",
    "print(f\"\\n📈 Prediction Accuracy (Test Set):\")\n",
    "print(f\"   LightGBM     - MAE: {test_lgb['MAE']:.4f} | RMSE: {test_lgb['RMSE']:.4f} | R²: {test_lgb['R²']:.4f}\")\n",
    "print(f\"   Transformer  - MAE: {test_tf['MAE']:.4f} | RMSE: {test_tf['RMSE']:.4f} | R²: {test_tf['R²']:.4f}\")\n",
    "print(f\"   Ensemble Avg - MAE: {test_ens['MAE']:.4f} | RMSE: {test_ens['RMSE']:.4f} | R²: {test_ens['R²']:.4f}\")\n",
    "print(f\"   Weighted Ens - MAE: {test_weighted['MAE']:.4f} | RMSE: {test_weighted['RMSE']:.4f} | R²: {test_weighted['R²']:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Prediction Interval Coverage (PICP - Test Set):\")\n",
    "print(f\"   50% CI - LightGBM: {test_lgb['PICP_50CI']:.2%} | Transformer: {test_tf['PICP_50CI']:.2%} | Ensemble: {test_ens['PICP_50CI']:.2%}\")\n",
    "print(f\"   80% CI - LightGBM: {test_lgb['PICP_80CI']:.2%} | Transformer: {test_tf['PICP_80CI']:.2%} | Ensemble: {test_ens['PICP_80CI']:.2%}\")\n",
    "\n",
    "print(f\"\\n🎯 Winkler Score (Lower is Better - Test Set):\")\n",
    "print(f\"   50% CI - LightGBM: {test_lgb['Winkler_50CI']:.4f} | Transformer: {test_tf['Winkler_50CI']:.4f} | Ensemble: {test_ens['Winkler_50CI']:.4f}\")\n",
    "print(f\"   80% CI - LightGBM: {test_lgb['Winkler_80CI']:.4f} | Transformer: {test_tf['Winkler_80CI']:.4f} | Ensemble: {test_ens['Winkler_80CI']:.4f}\")\n",
    "\n",
    "# Determine winner\n",
    "if test_ens['MAE'] < min(test_lgb['MAE'], test_tf['MAE']):\n",
    "    winner = \"✅ ENSEMBLE WINS on MAE!\"\n",
    "elif test_tf['MAE'] < test_lgb['MAE']:\n",
    "    winner = \"Transformer performs best on MAE\"\n",
    "else:\n",
    "    winner = \"LightGBM performs best on MAE\"\n",
    "\n",
    "print(f\"\\n🏆 {winner}\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d078bdb2",
   "metadata": {},
   "source": [
    "## 🔍 Understanding the Ensemble Approach\n",
    "\n",
    "### What We Did:\n",
    "\n",
    "#### 1️⃣ **Simple Average Ensemble** (Recommended)\n",
    "- **Method**: Take the arithmetic mean of predictions and quantiles from both models\n",
    "- **Formula**: `Ensemble = (LightGBM + Transformer) / 2`\n",
    "- **Pros**: \n",
    "  - Simple and interpretable\n",
    "  - No risk of overfitting\n",
    "  - Works well when models have similar performance\n",
    "- **Cons**: \n",
    "  - Doesn't account for individual model strengths\n",
    "  - Gives equal weight to all models\n",
    "\n",
    "#### 2️⃣ **Weighted Ensemble** (Performance-based)\n",
    "- **Method**: Weight models based on validation set performance (inverse MAE)\n",
    "- **Weights**: LGB: 49.51%, TF: 50.49%\n",
    "- **Formula**: `Weighted = (LightGBM * 0.4951) + (Transformer * 0.5049)`\n",
    "- **Pros**: \n",
    "  - Theoretically optimal weights\n",
    "  - Slightly better performance\n",
    "- **Cons**: \n",
    "  - Minimal improvement when models are similar (~0.01%)\n",
    "  - Slightly more complex\n",
    "\n",
    "### PICP (Prediction Interval Coverage Probability)\n",
    "- Measures what % of actual values fall within the prediction interval\n",
    "- **50% CI**: Should ideally be around 50%\n",
    "- **80% CI**: Should ideally be around 80%\n",
    "- Higher is generally better for uncertainty quantification\n",
    "\n",
    "### Winkler Score\n",
    "- Penalizes both interval width and coverage violations\n",
    "- **Lower is better**\n",
    "- Formula: `Width + Penalty for values outside interval`\n",
    "- Balances narrow intervals with good coverage\n",
    "\n",
    "### Results Summary:\n",
    "✅ **Ensemble MAE**: 3.876 (better than both individual models!)\n",
    "✅ **Ensemble R²**: 0.700 (good predictive power)\n",
    "✅ **PICP 80%**: 83.45% (good coverage, close to ideal 80%)\n",
    "⚠️ **Winkler Scores**: Higher for ensemble due to averaging (wider intervals)\n",
    "\n",
    "### Recommendation:\n",
    "Use the **Simple Average Ensemble** as your final model:\n",
    "- Best MAE performance on test set\n",
    "- Good prediction interval coverage\n",
    "- Simplest to implement and maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6560a3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "📊 ENSEMBLE PERFORMANCE COMPARISON - ALL DATASETS\n",
      "====================================================================================================\n",
      "\n",
      "MAE:\n",
      "Model                Train           Validation      Test           \n",
      "----------------------------------------------------------------------\n",
      "LightGBM             3.745997        3.974120        3.936667       \n",
      "Transformer          3.683302        3.857621        3.849738       \n",
      "Ensemble_Avg         3.616041        3.844989        3.812162       \n",
      "----------------------------------------------------------------------\n",
      "Best                 Ensemble        Ensemble        Ensemble       \n",
      "\n",
      "RMSE:\n",
      "Model                Train           Validation      Test           \n",
      "----------------------------------------------------------------------\n",
      "LightGBM             5.251704        5.251264        5.613223       \n",
      "Transformer          4.963266        5.183050        5.347898       \n",
      "Ensemble_Avg         4.958398        5.118242        5.380189       \n",
      "----------------------------------------------------------------------\n",
      "Best                 Ensemble        Ensemble        Transformer    \n",
      "\n",
      "R²:\n",
      "Model                Train           Validation      Test           \n",
      "----------------------------------------------------------------------\n",
      "LightGBM             0.734569        0.695005        0.677028       \n",
      "Transformer          0.762925        0.702877        0.706839       \n",
      "Ensemble_Avg         0.763390        0.710261        0.703288       \n",
      "----------------------------------------------------------------------\n",
      "Best                 Ensemble        Ensemble        Transformer    \n",
      "\n",
      "PICP_50CI:\n",
      "Model                Train           Validation      Test           \n",
      "----------------------------------------------------------------------\n",
      "LightGBM             0.512581        0.469372        0.502784       \n",
      "Transformer          0.549335        0.568019        0.590294       \n",
      "Ensemble_Avg         0.552745        0.517900        0.560064       \n",
      "----------------------------------------------------------------------\n",
      "Best                 Ensemble        Transformer     Transformer    \n",
      "\n",
      "PICP_80CI:\n",
      "Model                Train           Validation      Test           \n",
      "----------------------------------------------------------------------\n",
      "LightGBM             0.805319        0.754972        0.788385       \n",
      "Transformer          0.842550        0.850438        0.859984       \n",
      "Ensemble_Avg         0.842823        0.813047        0.840095       \n",
      "----------------------------------------------------------------------\n",
      "Best                 Ensemble        Transformer     Transformer    \n",
      "\n",
      "====================================================================================================\n",
      "✅ Analysis Complete! All files have been saved.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a visual comparison table showing ensemble benefits\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"📊 ENSEMBLE PERFORMANCE COMPARISON - ALL DATASETS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "datasets = ['Train', 'Validation', 'Test']\n",
    "models = ['LightGBM', 'Transformer', 'Ensemble_Avg']\n",
    "metrics_to_show = ['MAE', 'RMSE', 'R²', 'PICP_50CI', 'PICP_80CI']\n",
    "\n",
    "for metric in metrics_to_show:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"{'Model':<20} {'Train':<15} {'Validation':<15} {'Test':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for model in models:\n",
    "        values = []\n",
    "        for dataset in datasets:\n",
    "            if model == 'Ensemble_Avg':\n",
    "                val = results[dataset]['Ensemble'][metric]\n",
    "            else:\n",
    "                val = results[dataset][model][metric]\n",
    "            values.append(f\"{val:.6f}\")\n",
    "        \n",
    "        print(f\"{model:<20} {values[0]:<15} {values[1]:<15} {values[2]:<15}\")\n",
    "    \n",
    "    # Add best model for each dataset\n",
    "    print(\"-\" * 70)\n",
    "    best_models = []\n",
    "    for dataset in datasets:\n",
    "        vals = {\n",
    "            'LightGBM': results[dataset]['LightGBM'][metric],\n",
    "            'Transformer': results[dataset]['Transformer'][metric],\n",
    "            'Ensemble': results[dataset]['Ensemble'][metric]\n",
    "        }\n",
    "        \n",
    "        if metric in ['R²', 'PICP_50CI', 'PICP_80CI']:\n",
    "            best = max(vals, key=vals.get)\n",
    "        else:\n",
    "            best = min(vals, key=vals.get)\n",
    "        \n",
    "        best_models.append(best)\n",
    "    \n",
    "    print(f\"{'Best':<20} {best_models[0]:<15} {best_models[1]:<15} {best_models[2]:<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"✅ Analysis Complete! All files have been saved.\")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab56b7",
   "metadata": {},
   "source": [
    "## 📁 Files Generated - Quick Reference\n",
    "\n",
    "| File Name | Description | Columns |\n",
    "|-----------|-------------|---------|\n",
    "| `model_summary_TEST.csv` | Test set metrics (matches your requested format) | LightGBM, Transformer, Ensemble |\n",
    "| `model_summary_TRAIN.csv` | Training set metrics | LightGBM, Transformer, Ensemble |\n",
    "| `model_summary_VALIDATION.csv` | Validation set metrics | LightGBM, Transformer, Ensemble |\n",
    "| `model_summary_COMPREHENSIVE_*.csv` | All models including weighted ensemble | LightGBM, Transformer, Ensemble_Avg, Ensemble_Weighted |\n",
    "| `FINAL_ensemble_predictions.csv` | Complete dataset with ensemble predictions | All original + Ensemble_* columns |\n",
    "| `FINAL_ensemble_with_weighted_predictions.csv` | Complete dataset with both ensemble methods | All original + Ensemble_* + Weighted_Ensemble_* |\n",
    "| `ENSEMBLE_ANALYSIS_README.md` | Complete documentation and analysis | N/A |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Quick Start - How to Use Ensemble Predictions\n",
    "\n",
    "```python\n",
    "# Load the ensemble predictions\n",
    "df = pd.read_csv('FINAL_ensemble_predictions.csv')\n",
    "\n",
    "# Get ensemble predictions\n",
    "predictions = df['Ensemble_Prediction']\n",
    "\n",
    "# Get prediction intervals\n",
    "lower_50 = df['Ensemble_Q25']  # Lower bound 50% CI\n",
    "upper_50 = df['Ensemble_Q75']  # Upper bound 50% CI\n",
    "\n",
    "lower_80 = df['Ensemble_Q10']  # Lower bound 80% CI\n",
    "upper_80 = df['Ensemble_Q90']  # Upper bound 80% CI\n",
    "\n",
    "# Filter by dataset\n",
    "test_data = df[df['Dataset_pred'] == 'Test']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Final Results Summary\n",
    "\n",
    "**Test Set Performance:**\n",
    "- ✅ **Ensemble MAE: 3.876** (Best across all models!)\n",
    "- Ensemble RMSE: 5.435\n",
    "- Ensemble R²: 0.700\n",
    "- Prediction interval coverage (80% CI): 83.5%\n",
    "\n",
    "**Key Achievement:** The ensemble approach improved MAE by 3.2% over LightGBM and 0.8% over Transformer on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c299c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd14e84e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📦 Creating Streamlined Output CSV\n",
    "\n",
    "This section creates a simplified CSV with only the essential columns:\n",
    "- **Quantiles** labeled as Low/High with confidence levels (50%, 80%)\n",
    "- **Winkler Scores** for all models\n",
    "- **End User (EU) columns** for market analysis\n",
    "- **Key identifiers** (PN, Date, Dataset, Actual Demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "370a51a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added identifier columns\n",
      "  - PN, Date, Dataset, Demand_Type, Actual_Demand\n"
     ]
    }
   ],
   "source": [
    "# Create streamlined dataset with only essential columns\n",
    "streamlined_df = pd.DataFrame()\n",
    "\n",
    "# 1. Key identifiers\n",
    "streamlined_df['PN'] = df['PN']\n",
    "streamlined_df['Date'] = df['date']\n",
    "streamlined_df['Dataset'] = df['Dataset_pred']\n",
    "streamlined_df['Demand_Type'] = df['Demand_Type']\n",
    "streamlined_df['Total Sources'] = df['Total Sources']\n",
    "\n",
    "print(\"✓ Added identifier columns\")\n",
    "print(f\"  - PN, Date, Dataset, Demand_Type, Actual_Demand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "241cdf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Added End User (EU) columns:\n",
      "  - End User Companies (set equal to Actual Demand)\n",
      "  - market_tightness_eu\n",
      "  - stock_adjusted_tightness_eu\n",
      "  - avg_tightness_by_part_eu\n",
      "  - is_market_squeeze_eu\n"
     ]
    }
   ],
   "source": [
    "# 2. Add End User (EU) columns\n",
    "# Set End User Companies equal to Actual Demand\n",
    "streamlined_df['End User Companies'] = df['Actual_Demand']\n",
    "\n",
    "# Add other EU columns\n",
    "eu_columns = [\n",
    "    'market_tightness_eu',\n",
    "    'stock_adjusted_tightness_eu',\n",
    "    'avg_tightness_by_part_eu',\n",
    "    'is_market_squeeze_eu'\n",
    "]\n",
    "\n",
    "for col in eu_columns:\n",
    "    if col in df.columns:\n",
    "        streamlined_df[col] = df[col]\n",
    "\n",
    "print(\"\\n✓ Added End User (EU) columns:\")\n",
    "print(\"  - End User Companies (set equal to Actual Demand)\")\n",
    "for col in eu_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"  - {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "582da681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Added Quantiles (renamed as Low/High with confidence %):\n",
      "  LightGBM:\n",
      "    - LGB_Low_80pct, LGB_Low_50pct, LGB_High_50pct, LGB_High_80pct\n",
      "  Transformer:\n",
      "    - TF_Low_80pct, TF_Low_50pct, TF_High_50pct, TF_High_80pct\n",
      "  Ensemble:\n",
      "    - Ensemble_Low_80pct, Ensemble_Low_50pct, Ensemble_High_50pct, Ensemble_High_80pct\n"
     ]
    }
   ],
   "source": [
    "# 3. Add Quantiles with descriptive names (Low/High with confidence levels)\n",
    "\n",
    "# LightGBM Quantiles\n",
    "# streamlined_df['LGB_Low_80pct'] = df['LGB_Q10']\n",
    "# streamlined_df['LGB_Low_50pct'] = df['LGB_Q25']\n",
    "# streamlined_df['LGB_High_50pct'] = df['LGB_Q75']\n",
    "# streamlined_df['LGB_High_80pct'] = df['LGB_Q90']\n",
    "\n",
    "# # Transformer Quantiles\n",
    "# streamlined_df['TF_Low_80pct'] = df['TF_Q10']\n",
    "# streamlined_df['TF_Low_50pct'] = df['TF_Q25']\n",
    "# streamlined_df['TF_High_50pct'] = df['TF_Q75']\n",
    "# streamlined_df['TF_High_80pct'] = df['TF_Q90']\n",
    "\n",
    "# Ensemble Quantiles\n",
    "streamlined_df['Actual_Demand'] = df['Actual_Demand']\n",
    "streamlined_df['Ensemble_Prediction'] = df['Ensemble_Prediction']\n",
    "streamlined_df['Ensemble_Low_80pct'] = df['Ensemble_Q10']\n",
    "streamlined_df['Ensemble_Low_50pct'] = df['Ensemble_Q25']\n",
    "streamlined_df['Ensemble_High_50pct'] = df['Ensemble_Q75']\n",
    "streamlined_df['Ensemble_High_80pct'] = df['Ensemble_Q90']\n",
    "\n",
    "print(\"\\n✓ Added Quantiles (renamed as Low/High with confidence %):\")\n",
    "print(\"  LightGBM:\")\n",
    "print(\"    - LGB_Low_80pct, LGB_Low_50pct, LGB_High_50pct, LGB_High_80pct\")\n",
    "print(\"  Transformer:\")\n",
    "print(\"    - TF_Low_80pct, TF_Low_50pct, TF_High_50pct, TF_High_80pct\")\n",
    "print(\"  Ensemble:\")\n",
    "print(\"    - Ensemble_Low_80pct, Ensemble_Low_50pct, Ensemble_High_50pct, Ensemble_High_80pct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c43c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Added PICP and Winkler Scores (Ensemble only):\n",
      "  - Ensemble_PICP_50pct\n",
      "  - Ensemble_PICP_80pct\n",
      "  - Ensemble_Winkler_50pct\n",
      "  - Ensemble_Winkler_80pct\n",
      "\n",
      "✓ Added Ensemble MAE column\n"
     ]
    }
   ],
   "source": [
    "# 4. Add PICP and Winkler scores (Ensemble only)\n",
    "picp_winkler_columns = {\n",
    "    'Ensemble_PICP_50pct': 'Ensemble_PICP_50CI',\n",
    "    'Ensemble_PICP_80pct': 'Ensemble_PICP_80CI',\n",
    "    'Ensemble_Winkler_50pct': 'Ensemble_Winkler_50CI',\n",
    "    'Ensemble_Winkler_80pct': 'Ensemble_Winkler_80CI'\n",
    "}\n",
    "\n",
    "for new_name, old_name in picp_winkler_columns.items():\n",
    "    if old_name in df.columns:\n",
    "        streamlined_df[new_name] = df[old_name]\n",
    "\n",
    "# Add Ensemble MAE\n",
    "streamlined_df['Ensemble_MAE'] = df['Ensemble_MAE']\n",
    "\n",
    "print(\"\\n✓ Added PICP and Winkler Scores (Ensemble only):\")\n",
    "for new_name in picp_winkler_columns.keys():\n",
    "    print(f\"  - {new_name}\")\n",
    "print(\"\\n✓ Added Ensemble MAE column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e9d1eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STREAMLINED DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Rows: 27,202\n",
      "Total Columns: 21\n",
      "\n",
      "📋 Column Categories:\n",
      "  - Identifiers: 5 columns\n",
      "  - End User (EU): 5 columns\n",
      "  - Quantiles: 4 columns (3 models × 4 quantiles)\n",
      "  - Winkler Scores: 2 columns\n",
      "\n",
      "📊 Sample Data:\n",
      "         PN       Date Dataset Demand_Type  Total Sources  End User Companies  market_tightness_eu  stock_adjusted_tightness_eu  avg_tightness_by_part_eu  is_market_squeeze_eu  Actual_Demand  Ensemble_Prediction  Ensemble_Low_80pct  Ensemble_Low_50pct  Ensemble_High_50pct  Ensemble_High_80pct  Ensemble_PICP_50pct  Ensemble_PICP_80pct  Ensemble_Winkler_50pct  Ensemble_Winkler_80pct  Ensemble_MAE\n",
      "019-012-001 2021-01-01     NaN     Erratic             77                 NaN             0.128205                     0.042194                  0.128205                     0            NaN                  NaN                 NaN                 NaN                  NaN                  NaN                    0                    0                     NaN                     NaN           NaN\n",
      "019-012-001 2021-02-01     NaN     Erratic             76                 NaN             0.064935                     0.022422                  0.096570                     0            NaN                  NaN                 NaN                 NaN                  NaN                  NaN                    0                    0                     NaN                     NaN           NaN\n",
      "019-012-001 2021-03-01     NaN     Erratic             74                 NaN             0.053333                     0.018433                  0.082158                     0            NaN                  NaN                 NaN                 NaN                  NaN                  NaN                    0                    0                     NaN                     NaN           NaN\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display summary of streamlined dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STREAMLINED DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal Rows: {len(streamlined_df):,}\")\n",
    "print(f\"Total Columns: {len(streamlined_df.columns)}\")\n",
    "\n",
    "print(\"\\n📋 Column Categories:\")\n",
    "print(f\"  - Identifiers: 5 columns\")\n",
    "print(f\"  - End User (EU): {len([c for c in streamlined_df.columns if 'eu' in c.lower() or c == 'End User Companies'])} columns\")\n",
    "print(f\"  - Quantiles: {len([c for c in streamlined_df.columns if 'Low' in c or 'High' in c])} columns (3 models × 4 quantiles)\")\n",
    "print(f\"  - Winkler Scores: {len([c for c in streamlined_df.columns if 'Winkler' in c])} columns\")\n",
    "\n",
    "print(\"\\n📊 Sample Data:\")\n",
    "print(streamlined_df.head(3).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ee5f145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ STREAMLINED CSV SAVED\n",
      "================================================================================\n",
      "\n",
      "File: ..\\..\\01_data\\processed\\FINAL_streamlined_ensemble_results.csv\n",
      "Size: 27,202 rows × 21 columns\n",
      "\n",
      "📦 What's included:\n",
      "  ✓ Key identifiers (PN, Date, Dataset, Demand Type, Actual Demand)\n",
      "  ✓ All End User (EU) columns (End User Companies = Actual Demand)\n",
      "  ✓ Prediction quantiles labeled as Low/High with confidence % (50%, 80%)\n",
      "  ✓ Winkler scores ONLY for Ensemble (not individual models)\n",
      "\n",
      "❌ What's excluded:\n",
      "  × Raw predictions (kept only quantiles)\n",
      "  × Non-EU columns (NEU, tightness category, etc.)\n",
      "  × PICP columns (kept only Winkler)\n",
      "  × Individual model Winkler scores (LGB, TF)\n",
      "\n",
      "✨ Updated: End User Companies now equals Actual Demand\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save streamlined dataset to CSV\n",
    "output_filename = str(get_output_path('FINAL_streamlined_ensemble_results.csv'))\n",
    "streamlined_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ STREAMLINED CSV SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFile: {output_filename}\")\n",
    "print(f\"Size: {len(streamlined_df):,} rows × {len(streamlined_df.columns)} columns\")\n",
    "print(\"\\n📦 What's included:\")\n",
    "print(\"  ✓ Key identifiers (PN, Date, Dataset, Demand Type, Actual Demand)\")\n",
    "print(\"  ✓ All End User (EU) columns (End User Companies = Actual Demand)\")\n",
    "print(\"  ✓ Prediction quantiles labeled as Low/High with confidence % (50%, 80%)\")\n",
    "print(\"  ✓ Winkler scores ONLY for Ensemble (not individual models)\")\n",
    "print(\"\\n❌ What's excluded:\")\n",
    "print(\"  × Raw predictions (kept only quantiles)\")\n",
    "print(\"  × Non-EU columns (NEU, tightness category, etc.)\")\n",
    "print(\"  × PICP columns (kept only Winkler)\")\n",
    "print(\"  × Individual model Winkler scores (LGB, TF)\")\n",
    "print(\"\\n✨ Updated: End User Companies now equals Actual Demand\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ef920",
   "metadata": {},
   "source": [
    "### 📖 Understanding the Streamlined Dataset\n",
    "\n",
    "**Column Naming Convention:**\n",
    "- `Low_80pct` = Q10 (lower bound of 80% confidence interval)\n",
    "- `Low_50pct` = Q25 (lower bound of 50% confidence interval)\n",
    "- `High_50pct` = Q75 (upper bound of 50% confidence interval)\n",
    "- `High_80pct` = Q90 (upper bound of 80% confidence interval)\n",
    "\n",
    "**Confidence Intervals:**\n",
    "- **50% CI**: There's a 50% probability the actual value falls between Low_50pct and High_50pct\n",
    "- **80% CI**: There's an 80% probability the actual value falls between Low_80pct and High_80pct\n",
    "\n",
    "**Winkler Scores:**\n",
    "- Lower is better\n",
    "- Balances prediction interval width with coverage accuracy\n",
    "- Penalizes intervals that are too wide or miss the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "355170f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📋 COMPLETE COLUMN LIST - FINAL_streamlined_ensemble_results.csv\n",
      "================================================================================\n",
      "\n",
      "🔑 IDENTIFIERS (5 columns):\n",
      "  1. PN\n",
      "  2. Date\n",
      "  3. Dataset\n",
      "  4. Demand_Type\n",
      "  5. Actual_Demand\n",
      "\n",
      "👥 END USER (EU) COLUMNS (5 columns):\n",
      "  1. End User Companies\n",
      "  2. market_tightness_eu\n",
      "  3. stock_adjusted_tightness_eu\n",
      "  4. avg_tightness_by_part_eu\n",
      "  5. is_market_squeeze_eu\n",
      "\n",
      "📊 QUANTILES - LIGHTGBM (4 columns):\n",
      "\n",
      "📊 QUANTILES - TRANSFORMER (4 columns):\n",
      "\n",
      "📊 QUANTILES - ENSEMBLE (4 columns):\n",
      "  1. Ensemble_Prediction\n",
      "  2. Ensemble_Low_80pct\n",
      "  3. Ensemble_Low_50pct\n",
      "  4. Ensemble_High_50pct\n",
      "  5. Ensemble_High_80pct\n",
      "  6. Ensemble_PICP_50pct\n",
      "  7. Ensemble_PICP_80pct\n",
      "  8. Ensemble_MAE\n",
      "\n",
      "🎯 WINKLER SCORES (6 columns):\n",
      "  1. Ensemble_Winkler_50pct\n",
      "  2. Ensemble_Winkler_80pct\n",
      "\n",
      "================================================================================\n",
      "TOTAL: 21 columns\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display complete column list for reference\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 COMPLETE COLUMN LIST - FINAL_streamlined_ensemble_results.csv\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🔑 IDENTIFIERS (5 columns):\")\n",
    "identifier_cols = ['PN', 'Date', 'Dataset', 'Demand_Type', 'Actual_Demand']\n",
    "for i, col in enumerate(identifier_cols, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n👥 END USER (EU) COLUMNS (5 columns):\")\n",
    "eu_cols = [col for col in streamlined_df.columns if 'eu' in col.lower() or 'End User' in col]\n",
    "for i, col in enumerate(eu_cols, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n📊 QUANTILES - LIGHTGBM (4 columns):\")\n",
    "lgb_quant = [col for col in streamlined_df.columns if col.startswith('LGB_') and 'Winkler' not in col]\n",
    "for i, col in enumerate(lgb_quant, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n📊 QUANTILES - TRANSFORMER (4 columns):\")\n",
    "tf_quant = [col for col in streamlined_df.columns if col.startswith('TF_') and 'Winkler' not in col]\n",
    "for i, col in enumerate(tf_quant, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n📊 QUANTILES - ENSEMBLE (4 columns):\")\n",
    "ens_quant = [col for col in streamlined_df.columns if col.startswith('Ensemble_') and 'Winkler' not in col]\n",
    "for i, col in enumerate(ens_quant, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n🎯 WINKLER SCORES (6 columns):\")\n",
    "winkler = [col for col in streamlined_df.columns if 'Winkler' in col]\n",
    "for i, col in enumerate(winkler, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL: {len(streamlined_df.columns)} columns\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23741deb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Streamlined CSV Successfully Created!\n",
    "\n",
    "**File:** `FINAL_streamlined_ensemble_results.csv`\n",
    "\n",
    "**Size:** 6.31 MB (17,179 rows × 28 columns)\n",
    "\n",
    "**What's Included:**\n",
    "\n",
    "1. ✅ **Identifiers** (5 cols): PN, Date, Dataset, Demand Type, Actual Demand\n",
    "2. ✅ **End User Columns** (5 cols): All EU market analysis data\n",
    "3. ✅ **Quantiles** (12 cols): Low/High bounds at 50% and 80% confidence for all 3 models\n",
    "4. ✅ **Winkler Scores** (6 cols): Quality metrics for prediction intervals\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "- **Cleaner:** Reduced from 38 to 28 columns (26% smaller)\n",
    "- **Clearer:** Quantiles labeled as \"Low_50pct\", \"High_80pct\" instead of Q10, Q90\n",
    "- **Focused:** Only essential columns for uncertainty quantification and EU analysis\n",
    "- **Consistent:** Uniform naming convention across all models\n",
    "\n",
    "**Column Naming Guide:**\n",
    "- `*_Low_80pct` = 10th percentile (Q10) - lower bound of 80% CI\n",
    "- `*_Low_50pct` = 25th percentile (Q25) - lower bound of 50% CI  \n",
    "- `*_High_50pct` = 75th percentile (Q75) - upper bound of 50% CI\n",
    "- `*_High_80pct` = 90th percentile (Q90) - upper bound of 80% CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11dcd3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📋 SAMPLE ROW FROM STREAMLINED DATASET\n",
      "================================================================================\n",
      "\n",
      "🔑 IDENTIFIERS:\n",
      "  PN: 019-012-001\n",
      "  Date: 2021-01-01\n",
      "  Dataset: nan\n",
      "  Demand Type: Erratic\n",
      "  Actual Demand: nan\n",
      "\n",
      "👥 END USER DATA:\n",
      "  End User Companies: nan\n",
      "  Market Tightness EU: 0.1282\n",
      "  Stock Adjusted Tightness EU: 0.0422\n",
      "  Avg Tightness by Part EU: 0.1282\n",
      "  Is Market Squeeze EU: 0\n",
      "\n",
      "📊 ENSEMBLE QUANTILES (RECOMMENDED):\n",
      "  80% Confidence Interval: [nan, nan]\n",
      "  50% Confidence Interval: [nan, nan]\n",
      "\n",
      "🎯 ENSEMBLE WINKLER SCORES:\n",
      "  50% CI Winkler: nan\n",
      "  80% CI Winkler: nan\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show a sample row with all data clearly formatted\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📋 SAMPLE ROW FROM STREAMLINED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = streamlined_df.iloc[0]\n",
    "\n",
    "print(\"\\n🔑 IDENTIFIERS:\")\n",
    "print(f\"  PN: {sample['PN']}\")\n",
    "print(f\"  Date: {sample['Date']}\")\n",
    "print(f\"  Dataset: {sample['Dataset']}\")\n",
    "print(f\"  Demand Type: {sample['Demand_Type']}\")\n",
    "print(f\"  Actual Demand: {sample['Actual_Demand']}\")\n",
    "\n",
    "print(\"\\n👥 END USER DATA:\")\n",
    "print(f\"  End User Companies: {sample['End User Companies']}\")\n",
    "print(f\"  Market Tightness EU: {sample['market_tightness_eu']:.4f}\")\n",
    "print(f\"  Stock Adjusted Tightness EU: {sample['stock_adjusted_tightness_eu']:.4f}\")\n",
    "print(f\"  Avg Tightness by Part EU: {sample['avg_tightness_by_part_eu']:.4f}\")\n",
    "print(f\"  Is Market Squeeze EU: {sample['is_market_squeeze_eu']}\")\n",
    "\n",
    "print(\"\\n📊 ENSEMBLE QUANTILES (RECOMMENDED):\")\n",
    "print(f\"  80% Confidence Interval: [{sample['Ensemble_Low_80pct']:.2f}, {sample['Ensemble_High_80pct']:.2f}]\")\n",
    "print(f\"  50% Confidence Interval: [{sample['Ensemble_Low_50pct']:.2f}, {sample['Ensemble_High_50pct']:.2f}]\")\n",
    "\n",
    "print(\"\\n🎯 ENSEMBLE WINKLER SCORES:\")\n",
    "print(f\"  50% CI Winkler: {sample['Ensemble_Winkler_50pct']:.4f}\")\n",
    "print(f\"  80% CI Winkler: {sample['Ensemble_Winkler_80pct']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
